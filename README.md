# Dymension

The problem to solve is defined in [description.pdf](description.pdf) file.

## Data store

The goal of the task is to design mechanism for synchronizing decentralized data storage.
But it does not define the structure of that store. Is it a filesystem, key-value store, database?

For the purpose of this design I assume that the data store is a binary blob of some fixed size.
This format might represent a large set of potential use cases. At the end of the day any data
are stored as byte blobs on some persistent block device.

The space is partitioned into N blocks, each containing 4KB of data. 4KB is selected because most of the
commonly used SSD drives today use that block size, meaning that reads and writes using that block size are most effective.

Each 4KB block in the blob is addressed using its index, which is the number in the range [0, N-1].
That way, it is efficient to access any block on the block device by using `seek` operation.
If the index is an unsigned 64-bit integer, it is possible to address 67108864 petabytes of data using this system.

## Proofs

System must provide data integrity. For that purpose, next to the data, merkle tree is stored, containing the hashes of the 4KB blocks.
There are two types of nodes in that tree:
- leaf nodes - containing a set of signatures generated by the sequencer for data blocks referenced by the leaf node (one signature per each data block)
- pointer nodes - containing hashes of leaf nodes or pointer nodes of the lower level

At the top, root node exists, containing the hash of the entire merkle tree.

Also, the list of all the root hashes must be stored, to be able to prove the data integrity
from the genesis state.

## Sequencer operation

Operation is a request to update the data store, produced by the sequencer every 0.2 seconds.
In the spec the term block is used for this purpose, but it collides with the same term I used above.

There are 150 operations (30 / 0.2) in every batch. For the purpose of this design I assumed that it is very unlikely
to observe two operations in the same batch touching the same data block.

Each operation broadcasted by a sequencer modifies some number of consequtive bytes in a single block.
The operation consists of:
- block index
- offset in the block
- bytes to write (only the modified ones, not the entire data block)
- signature of the entire block (after the operation) 

When the operation defined this way is executed by the full node:
1. block is read from the device (`seek` + `read`) and stored in the memory (4KB of data)
2. appropriate bytes are modified
3. sequencer's signature is verified using its public key 
4. block is written to the block device (`write`)
5. appropriate nodes in the merkle tree are recalculated

## Light client queries

When the light client needs to access the data, two variants are possible:
- light client trusts the full node
- light client doesn't trust the full node

In the trustful setup, the client simply asks the full node to respond with a scope of bytes
defined by the block index, offset and count.

In the trustless setup, the client must request full 4KB data block and all the relevant nodes
from the merkle tree, to be able to compute the proof of the state. The liht client might also track
or query for all the previous merkle tree hashes up to the genesis state. But it is more common to set
the trusted hash of some recent block when the light client is started for the first time.

## Updates optimization

It is defined that single operation is produced every 0.2 second but persistent state is generated once per 30 seconds,
aggregating all the operations from that period. It means, there are 150 operations to execute every 30 seconds.

The naive approach would be to aggregate all the operations inside the sequencer and then broadcast them at the end of the period.
That would mean that for 30 seconds the network does nothing, and then it is saturated by transferring tons of data.

Far better approach is to broadcast each operation immediately and let full nodes execute them as they come.
In this case full nodes must deal with temporary state (generated by the current batch) and persistent state
(generated at the end of the batch) used to generate responses for the requests received from the light clients.

To do that efficiently, the mechanism of snapshots must be implemented. Snapshotting is the huge topic on its own,
so I'm only mentioning here that BTRFS and ZFS filesystems are the good implementations.
For the purpose of this example, the data blob could be stored on such filesystem and snapshot could be taken every 150 operations
and mounted to serve the queries coming from the light clients.